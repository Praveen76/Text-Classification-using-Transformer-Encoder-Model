{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveen76/Text-Classification-using-Transformer-Encoder-Model/blob/main/Text_Classification_using_Transformer_Encoder_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the big picture of transformers\n",
        "* understand and work with the Textvectorization layer\n",
        "* understand and work with the embedding layer\n",
        "* understand the consept of self attention\n",
        "* explore transformer encoder and positional embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Big Picture"
      ],
      "metadata": {
        "id": "Tv8TQrtfwaNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/TransformerArch.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121224Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=2b77821d251395615c756f497bb30d763453f0cbeb71ebe1cb390d1ff80a7462\" width=800px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "ni6YCBwE7d_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the entire architecture of transformer. A TextVectorization layer, Embedding layer, an Encoder and a Decoder.\n",
        "\n",
        "Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence."
      ],
      "metadata": {
        "id": "AxzUg64Jdv-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer architecture was originally designed for translation. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
        "\n",
        "To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3."
      ],
      "metadata": {
        "id": "rgVPmcTjnORs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/TransformerArch.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121224Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=2b77821d251395615c756f497bb30d763453f0cbeb71ebe1cb390d1ff80a7462\" width=900px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "VtUillZR7eEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment decoder will not form the topic of discussion, the main focus will be on the Transformer Encoder.\n",
        "This has been discussed in detail in the later sections of this notebook."
      ],
      "metadata": {
        "id": "geRywC9LeHjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description"
      ],
      "metadata": {
        "id": "ECysu18IoMaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews."
      ],
      "metadata": {
        "id": "XLrl_aUaoPam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is processed and used in the later sections of this notebook."
      ],
      "metadata": {
        "id": "KzZ2UaHyoS6z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os, pathlib, shutil, random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import text_dataset_from_directory"
      ],
      "metadata": {
        "id": "qnUhfGmx9cup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Part A** : Text Pre-processing and Embedding Before Transformer Block"
      ],
      "metadata": {
        "id": "4F75jcgTzupQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextVectorization"
      ],
      "metadata": {
        "id": "KtDuXYqa7cbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the text data:\n",
        "  * Text standardization\n",
        "  * Text splitting\n",
        "  * Vocabulary indexing\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "TIv-Yv3EnQ4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A flowchart depicting the procedure or sequence of steps followed by a TextVectorization layer. 'Standardization' is taking care of basic preprocessing of text data such as removing the punctuation and converting the text to lower case. 'Tokenization' is giving the list of words from the sentence. Later, these words are represented with indices and with the help of embedding to get the vector encoding of indices."
      ],
      "metadata": {
        "id": "o590tI2BldJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Transformer_Encoder_Text_data_prep.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121307Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=480bbabbe19ea3ae96b4fca9c7bf98e5330baab47382b770c6e314ac9ce6d006\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "y-m7SHDzoHHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All these steps are performed in a TextVectorization Layer.\n",
        "\n",
        "\n",
        "*   Keras provides a TextVectorization layer which can be dropped directly into\n",
        "      - a tf.data pipeline **or**\n",
        "      - a Keras model\n",
        "\n",
        "*  MOREOVER, TextVectorization also handles both approaches of representing groups of words:\n",
        "      - Words as a set or Bag-of-words\n",
        "      - Words as a sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_RGm-7ySooG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a dummy dataset and a test sentence\n"
      ],
      "metadata": {
        "id": "8QdHTVk5K_wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "# dataset_t = [\"I write, rewrite, and still rewrite again\"]\n",
        "#Q: Is the word 'still' in the dataset (vocabulary)? Is it there in the test_sentence?\n",
        "#Q: How many words in test_sentence?"
      ],
      "metadata": {
        "id": "njuS3JqcS-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function depicting TextVectorization layer"
      ],
      "metadata": {
        "id": "M9sp6faVpYYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function is defined here to demonstrate the working of a TextVectorization layer. This function also compares two text data by making use of encodings and decodings."
      ],
      "metadata": {
        "id": "pB0aHkyhr3D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will be called multiple times in the sub-sequent code cells to demonstrate TextVectorization with different parameters such as a combinations of monograms, bigrams and different modes of output."
      ],
      "metadata": {
        "id": "mwf1NCofvA_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the workings of TextVectorization\n",
        "def demonstrate_TxVec(text_vectorization, dataset, test_sen, mode=None):\n",
        "  # arguments:\n",
        "  text_vectorization.adapt(dataset) # Computes a vocabulary of string terms from tokens in a dataset\n",
        "  vocabulary = text_vectorization.get_vocabulary()\n",
        "  print(f\"vocabulary = {vocabulary}\")\n",
        "  print(f\"len(vocabulary) = {len(vocabulary)}\")\n",
        "\n",
        "  # To see how the the text_vec layer transforms/vectorizes the raw text\n",
        "  encoded_sentence = text_vectorization(test_sen)\n",
        "  print(f\"encoded sentence = {encoded_sentence}\")\n",
        "  print(f\"len(encoded sentence) = {len(encoded_sentence)}\")\n",
        "  # print(f\"encoded dataset_t = {text_vectorization(dataset_t)}\")\n",
        "\n",
        "  # decode back for comparison with test_sentence\n",
        "  if mode==\"int\":\n",
        "    inverse_vocab = dict(enumerate(vocabulary)) # making a dictionary to decode embeddings\n",
        "    print(f\"inverse_vocab = {inverse_vocab}\")\n",
        "    decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "    print(f\"decoded sentence = {decoded_sentence}\")\n",
        "\n",
        "  if mode==\"multi_hot\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  if mode==\"count\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  if mode==\"tf\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  print(f\"test_sentence = {test_sen}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eqL6sCaWTnTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer"
      ],
      "metadata": {
        "id": "IMKHL4rarg_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q: What 3 things does a TV layer do?\n",
        "# instantiating a TextVectorization layer/object\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",  # int is defualt. We will see different kinds of modes\n",
        "    # we can use custom fucntions for standardizing and splitting the text - see Chollet\n",
        "    # standardize=custom_standardization_fn,\n",
        "    # split=custom_split_fn,\n",
        ")\n",
        "\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"int\")\n",
        "\n",
        "# Q: What is a vocabulary?\n",
        "# Q: No. of tokens in vocabulary?\n",
        "# Q: Length of encoded_sentence (output of TV layer)?\n",
        "# Q: Type of elements in encoded_sentence (embedding)?\n",
        "# Q: Is decoded sentence the same as the test_sentence? Why?\n",
        "\n",
        "# dataset = [\n",
        "#     \"I write, erase, rewrite\",\n",
        "#     \"Erase again, and then\",\n",
        "#     \"A poppy blooms.\",\n",
        "# ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0AS4G6RqMEU",
        "outputId": "3a486d2d-bb7e-4e6e-b3ef-2843019e273c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 12\n",
            "encoded sentence = [ 7  3  5  9  1  5 10]\n",
            "len(encoded sentence) = 7\n",
            "inverse_vocab = {0: '', 1: '[UNK]', 2: 'erase', 3: 'write', 4: 'then', 5: 'rewrite', 6: 'poppy', 7: 'i', 8: 'blooms', 9: 'and', 10: 'again', 11: 'a'}\n",
            "decoded sentence = i write rewrite and [UNK] rewrite again\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer and considering bigrams"
      ],
      "metadata": {
        "id": "yd_qVVlIvr5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with integer encoding\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    output_mode=\"int\",\n",
        "    # output_sequence_length=20\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"int\")\n",
        "# Q: Can we have 'rewrite erase' in vocab?\n",
        "# Q: Why the extra 'i write'?\n",
        "# Q: Why the extra 'UNK' ?\n",
        "\n",
        "# dataset = [\n",
        "#     \"I write, erase, rewrite\",\n",
        "#     \"Erase again, and then\",\n",
        "#     \"A poppy blooms.\",\n",
        "# ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jepK4bjqWq1D",
        "outputId": "732644b1-9704-492b-e1b1-981d167a8fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['', '[UNK]', 'erase', 'write erase', 'write', 'then', 'rewrite', 'poppy blooms', 'poppy', 'i write', 'i', 'erase rewrite', 'erase again', 'blooms', 'and then', 'and', 'again and', 'again', 'a poppy', 'a']\n",
            "len(vocabulary) = 20\n",
            "encoded sentence = [10  4  6 15  1  6 17  9  1  1  1  1  1]\n",
            "len(encoded sentence) = 13\n",
            "inverse_vocab = {0: '', 1: '[UNK]', 2: 'erase', 3: 'write erase', 4: 'write', 5: 'then', 6: 'rewrite', 7: 'poppy blooms', 8: 'poppy', 9: 'i write', 10: 'i', 11: 'erase rewrite', 12: 'erase again', 13: 'blooms', 14: 'and then', 15: 'and', 16: 'again and', 17: 'again', 18: 'a poppy', 19: 'a'}\n",
            "decoded sentence = i write rewrite and [UNK] rewrite again i write [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer, maximum tokens as 20 and output mode as `'multi_hot'` encodings"
      ],
      "metadata": {
        "id": "dEz-sSvJvyoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Unigrams with binary encoding\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1, # Default value\n",
        "    max_tokens = 20, # let's change this value to 8 and see\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"multi_hot\")\n",
        "\n",
        "# Recall we saw this in tutorial 2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UXYhaDYF9xg",
        "outputId": "0610ce4e-6fd5-4a15-a1d4-d0e70fbf9a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'multi_hot'` and considering bigrams"
      ],
      "metadata": {
        "id": "-fR0OClawQIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2, # bag of 2 or less words. See cell o/p\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "# Bi-grams typically perform better than unigrams- Word order matters!\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"multi_hot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJnKVKRhGk-E",
        "outputId": "ae6eca59-7e2d-4346-f23b-536de54bbf93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write erase', 'write', 'then', 'rewrite', 'poppy blooms', 'poppy', 'i write', 'i', 'erase rewrite', 'erase again', 'blooms', 'and then', 'and', 'again and', 'again', 'a poppy', 'a']\n",
            "len(vocabulary) = 19\n",
            "encoded sentence = [1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "len(encoded sentence) = 19\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write erase:\t [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "poppy blooms:\t [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "i write:\t [0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase rewrite:\t [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase again:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "and then:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "again and:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "a poppy:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'count'` and considering monograms"
      ],
      "metadata": {
        "id": "_X9DrVGTweV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with token counts\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1,\n",
        "    # max_tokens = 8,\n",
        "    output_mode=\"count\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"count\")\n",
        "#Q: which token comes twice in the test_sentence?\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_itfCn0aGreY",
        "outputId": "13d00e7a-1455-46eb-d3d6-0446b08f8dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7ac55b75a7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [1. 0. 1. 0. 2. 0. 1. 0. 1. 1. 0.]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'tf_idf'` and considering monograms"
      ],
      "metadata": {
        "id": "ywFrwXxLwlD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with TF-IDF weighted outputs\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1,\n",
        "    output_mode=\"tf_idf\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"tf\")\n",
        "\n",
        "# Often leads to 1% increase in perfermonance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3rAdK92Gyfq",
        "outputId": "974b24c3-1cc9-40ac-9f63-239921f701f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7ac55b75b9a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [0.8939764  0.         0.91629076 0.         1.8325815  0.\n",
            " 0.91629076 0.         0.91629076 0.91629076 0.        ]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [0.8939764 0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.       ]\n",
            "erase:\t [0.        0.6931472 0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.       ]\n",
            "write:\t [0.         0.         0.91629076 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "then:\t [0.         0.         0.         0.91629076 0.         0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "rewrite:\t [0.         0.         0.         0.         0.91629076 0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "poppy:\t [0.         0.         0.         0.         0.         0.91629076\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "i:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.91629076 0.         0.         0.         0.        ]\n",
            "blooms:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.91629076 0.         0.         0.        ]\n",
            "and:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.91629076 0.         0.        ]\n",
            "again:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.91629076 0.        ]\n",
            "a:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.91629076]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextVectorization in Keras Model\n",
        "\n",
        "This technique is useful for production: For a stand-alone model.\n",
        "\n",
        "Load a saved model and add a 'text_vectorization' layer to it"
      ],
      "metadata": {
        "id": "yCQHEwZkFXmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a model architecture in a function"
      ],
      "metadata": {
        "id": "6QOfDPiNjTfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The processed inputs from will be later fed to this model."
      ],
      "metadata": {
        "id": "lyQ2i5GIWxDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense network which may be used repetitively\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                    loss=\"binary_crossentropy\",\n",
        "                    metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "TdyrQSXQjZEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  model=get_model()\n",
        "  inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "  processed_inputs = text_vectorization(inputs)\n",
        "  outputs = model(processed_inputs) #some trained model\n",
        "  inference_model = keras.Model(inputs, outputs)\n",
        "  inference_model.summary()\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "ImGc3tyXLgfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A discussion related to the method depicted above will be demonstrated in the next assignment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ILbGkb8LFYCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation Example"
      ],
      "metadata": {
        "id": "UkoDTKmd900K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pre-processed version of the IMDB dataset provided by Keras was used in the previous assignments."
      ],
      "metadata": {
        "id": "3XIAoz7enlj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally IMDB dataset contains the *train* and the *test* folders.\n",
        "Here, the original dataset will be used and pre-processing related to it will be explored."
      ],
      "metadata": {
        "id": "xcKJhCGE1yTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List subdirectories\n",
        "!cd aclImdb && ls -d */"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46rYfHyZrqbs",
        "outputId": "6f752ffa-e5c4-4e25-c9a2-e07cb80a40f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test/  train/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary folder\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "mFgcefQNLh-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise a sample\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DWQXmaYLuha",
        "outputId": "a4b5a52b-7836-4964-d43b-d3a3fd91afac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a validation directory and move 20% of the train data to it"
      ],
      "metadata": {
        "id": "Za7BufC_2q_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# move 20% of the training data to the validation folder\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    # random.Random(1337).shuffle(files) # We should shuffle. Only commenting for demonstration\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "metadata": {
        "id": "9BseO3_sLukB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create batches of data using `text_dataset_from_directory`"
      ],
      "metadata": {
        "id": "YBeeGi5S8pbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset using utility\n",
        "batch_size = 32\n",
        "\n",
        "# Q: Name other such utilities seen earlier ?\n",
        "train_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size)\n",
        "\n",
        "val_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size)\n",
        "\n",
        "test_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size)\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) #replace x,y with x. That is remove labels, just keep text data.\n"
      ],
      "metadata": {
        "id": "cDLoEh2CLwB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ec89a2-72cc-466c-d86c-583f198cae9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 20000, 5000, and 25000 records in train, validation, and test directories with two class as positive and negative."
      ],
      "metadata": {
        "id": "htOQeKY98_3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "\n",
        "    print(\"inputs[2]:\", inputs[2])\n",
        "    print(\"targets[2]:\", targets[2])\n",
        "    break"
      ],
      "metadata": {
        "id": "u_03-Oj9LwD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f325adbc-3639-49ba-c460-f9f949310a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[2]: tf.Tensor(b'Although I\\'m grateful this obscure gem of 70\\'s Italian exploitation cinema features in the recently released \"Grindhouse Experience\" box set, and although it\\'s also available on disc under the misleading and stupid alternate title \"Escape from Death Row\", I honestly think it deserves a proper and luxurious DVD edition, completely in its originally spoken languages with subtitle options (the dubbing is truly horrible), restored picture quality and a truckload of special bonus features! Heck, I don\\'t even need the restored picture quality and bonus features if only we could watch the film in its original language. \"Mean Frank and Crazy Tony\" is a cheerfully fast-paced mafia/crime flick with a lot of violence, comedy (which, admittedly, doesn\\'t always work), feminine beauty and two witty main characters. Tony Lo Bianco is terrific as the small thug pretending to be the city\\'s biggest Don. When the real crime lord Frankie Dio (Lee Van Cleef) arrives in town, he sees an opportunity to climb up the ladder by offering his services. Frankie initially ignores the little crook, but they do eventually form an unlikely team when Frankie\\'s entire criminal empire turns against him and a new French criminal mastermind even assassinates Frankie\\'s innocent brother. Tony helps Frankie to escape from prison and together they head for Marseille to extract Frankie\\'s revenge. The script of this sadly neglected crime gem funnily alters gritty action & suspense with light-headed bits of comedy, like the grotesque car chase through the narrow French mountain roads for example. The build up towards the typical mafia execution sequences (guided by an excellent Riz Ortolani score) are extremely tense and the actual killings are sadistic and merciless, which is probably why the film is considered to be somewhat of a grindhouse classic. The film lacks a strong female lead, as the lovely and amazingly voluptuous beauty Edwige Fenech sadly just appears in a couple of scenes, and then still in the background. On of the men behind the camera, responsible for the superb cinematography, was no less then Joe D\\'Amato. Great film, highly recommended to fans of Italian exploitation, and I hope to watch it again soon in its original version.', shape=(), dtype=string)\n",
            "targets[2]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the data using TextVectorization layer of keras"
      ],
      "metadata": {
        "id": "sR63E5Ea9fDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the data\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,    # Q: What is the vocabular size?\n",
        "    output_mode=\"int\",        # Q: What will be the type of output for a token (say), 'amazing' ?\n",
        "    output_sequence_length=max_length,      # Q: What is the maximum length of review? Is it a fair assumption?\n",
        "    )\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bqi5Z24gMK9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Transformer_Encoder_Text_data_prep.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121307Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=480bbabbe19ea3ae96b4fca9c7bf98e5330baab47382b770c6e314ac9ce6d006\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "lCFPfVAts1IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize and compare the raw and processed data"
      ],
      "metadata": {
        "id": "J7sj-tcc9v2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the raw text and the vectorized (to int) text\n",
        "for text, label in train_ds:\n",
        "  print(text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "for int_of_text, label in int_train_ds:\n",
        "  print(int_of_text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "# Q: How can you verify whether the index of movie is 18?\n"
      ],
      "metadata": {
        "id": "nMUYGUqBx3K1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f26129-4200-40f8-8fb3-3a0a68566759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'I\\'m easily entertained. I enjoyed \"Hot Shots\" and \"The Naked Gun\" and their many sequels, even when most people found them unbearable. I\\'ve even managed to enjoy most Pauly Shore movies. There is only one movie that I\\'ve seen that I can honestly say was bad...and this was it. It\\'s been a while since I\\'ve seen it, but I do remember sitting in the theater thinking, \"This is a dumb movie. Why did I see this?\" It\\'s honestly the only movie that I cannot recommend.', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(\n",
            "[    2  1175    11    20   416     1     9   449     6 18753   137    21\n",
            "    16   122     2    51  3215    10   832    15   174  2035    49     4\n",
            "   424     5    63  1224   141    11   950  1944    87    21   702    85\n",
            "    80    77     6    95    96    39     1   854  4047 11559    16   136\n",
            "  1976    90    28  4397    33     2  2259   319  3047    21   935    15\n",
            "     9    41  1483    23    69  2429    47     5     2    80    32     2\n",
            "    63     3     8    11   432     2   273  1751    26    34  2886    17\n",
            "  1888    38     9     7    22   861     6   162    46     2    97    11\n",
            "    18    76  1062  5443   936    69   110   373    12     2   347     5\n",
            "     2   189   617     9  1159    87   943     8     2  2773    44   136\n",
            "    34  9372     3  3814  1099  1276    13    13     1    13     2   469\n",
            "   287  2210     4  3486    37  2970    79     4   997   189     5   107\n",
            "     4  2448     8    34  2609     1   257   610     4   222    17   108\n",
            "  3870  7727   476     4  1633   251     3   436   588   444     5    33\n",
            "     4  6614   685  4126   342    37    69  1146  6119     2  2448     7\n",
            " 10699    33     2  1633   251    42 18632     3     2 12389   436     7\n",
            "  1480   588     8    33     2 16068     5     2     1    13     2  2448\n",
            "  2455     2   212    63     8     4  1134     5     1     3     1   950\n",
            "  3486    37  8897     6   137     6     2  2926    91   162    16   563\n",
            "     3    91  6785     4     1    21    25   222    15    27     7   100\n",
            "  2917     6    95   280     1    13   897  8419    13    37   467     6\n",
            "   104   352     4   609  1197  3486     8     4   997   189  2184     4\n",
            "   341   605    42   854   673     4   339   222    37    24     8   986\n",
            "    22    73   124    71   442     1    13    11    20     7  1151 17226\n",
            "    13     9    60  3109   402    36   376     2  4764   975   639    33\n",
            "    47   358  1606   492  7981    13  4110     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of the word movie"
      ],
      "metadata": {
        "id": "P01-UwZIAuzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization(\"movie\")\n",
        "# Q: What is the shape of the TV output?\n",
        "# Q: Why so many 0s?\n"
      ],
      "metadata": {
        "id": "Jl8iSY55z1A0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8048c19f-e47d-456d-e26b-a14d2faf6456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(600,), dtype=int64, numpy=\n",
              "array([18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0])>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of \"great movie\" and \"a fine story\""
      ],
      "metadata": {
        "id": "6oAL6gzUAzvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization([\"great movie\",\"a fine story\"])\n",
        "#Q: shape?"
      ],
      "metadata": {
        "id": "oCDXUf8bDRIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f10feb2-a1b8-4859-b520-99f174fa359f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 600), dtype=int64, numpy=\n",
              "array([[ 84,  18,   0, ...,   0,   0,   0],\n",
              "       [  4, 471,  64, ...,   0,   0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "BL3w3DRqVy5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Word embeddings are vector representations of words that achieve exactly this: they map human language into a structured geometric space.\n",
        "\n",
        "* dense (floats)\n",
        "* low-dimensional (1024 dims for large vocabs)"
      ],
      "metadata": {
        "id": "ex3lQf00YyM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "* Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors, in the same way you learn the weights of a neural network. **Move away from manual feature engineering.**\n",
        "* Load into your model word embeddings that were precomputed using a different machine learning task than the one youre trying to solve. These are called pretrained word embeddings.\n",
        "\n",
        "**Q: Do two ways remind you of something we studied in CNNs ?**\n",
        "\n",
        "In this assignment the main agenda is to explore the Learning of word embeddings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fMKg5tOpcgXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layer\n"
      ],
      "metadata": {
        "id": "t5OVct9NckRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The procedure if as follows:\n",
        "\n",
        "*   Like a dictionary that **maps integer indices** (which stand for specific words) **to dense vectors**\n",
        "\n",
        "*   Input: a rank-2 tensor of integers, of shape (batch_size, sequence_length)\n",
        "*   Output: 3D floating-point tensor of shape (batch_size, sequence_length, embedding_dimensionality)\n",
        "*   WORD INDEX  EMBEDDING LAYER  CORRESPONDING WORD VEC\n",
        "\n",
        "*   Initial weights are random\n",
        "*   Learns specialized structure upon training\n",
        "\n"
      ],
      "metadata": {
        "id": "gdLRIpXwkQAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Chart.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121403Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=d09dbf92b62259091b89f11a826bb14c78750b8445ffc1a74f56c4220e920c6e\" width=750px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "G91_UEMy-gpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define an LSTM architecture with an Embedding layer, and a Bidriectional layer"
      ],
      "metadata": {
        "id": "K_tTVNb4GhCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 20000\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "# The Embedding layer\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)  # the largest integer (i.e. word index) in the input\n",
        "                                                                             # should be no larger than 19999 (vocabulary size).\n",
        "# Q: What is the input to the Embedding layer?\n",
        "# Q: What is the dimension of the output embeddings\n",
        "# Q: In embedding layer shape, what are None and None ?\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "#Q: Weights in the embedding layer?\n",
        "#Hint: Dict; 1 input word => embedding of size ___ ."
      ],
      "metadata": {
        "id": "njaeQikslGr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8c5748-3a5e-4015-985b-087ee82ad72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                73984     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5194049 (19.81 MB)\n",
            "Trainable params: 5194049 (19.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model and make a prediction"
      ],
      "metadata": {
        "id": "0wwNyu9PG4Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model - This code cell can be commented for brevity\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "fPbyk2A483fF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405fb3e4-ce76-4871-8d45-33a73fb8e8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 64s 93ms/step - loss: 0.5259 - accuracy: 0.7401 - val_loss: 0.4061 - val_accuracy: 0.8332\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.3517 - accuracy: 0.8688 - val_loss: 0.3377 - val_accuracy: 0.8612\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.2864 - accuracy: 0.8958 - val_loss: 0.3556 - val_accuracy: 0.8462\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.2317 - accuracy: 0.9190 - val_loss: 0.3543 - val_accuracy: 0.8632\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.1993 - accuracy: 0.9321 - val_loss: 0.4052 - val_accuracy: 0.8494\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.1709 - accuracy: 0.9426 - val_loss: 0.4044 - val_accuracy: 0.8550\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.1538 - accuracy: 0.9496 - val_loss: 0.4586 - val_accuracy: 0.8504\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.1314 - accuracy: 0.9590 - val_loss: 0.4274 - val_accuracy: 0.8708\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 28s 44ms/step - loss: 0.1115 - accuracy: 0.9660 - val_loss: 0.5808 - val_accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.0948 - accuracy: 0.9735 - val_loss: 0.4769 - val_accuracy: 0.8692\n",
            "782/782 [==============================] - 16s 19ms/step - loss: 0.3478 - accuracy: 0.8567\n",
            "Test acc: 0.857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **Part B** : Building Encoder Transformer"
      ],
      "metadata": {
        "id": "L4lbyeZVxojj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure below shows the **Transformer Model Architecture** as per the paper [\"**Attention Is All You Need**\"](https://arxiv.org/pdf/1706.03762v6.pdf) .Here, we are going to implement **Encoder** and try to understand how it function. We are writing **'as per the paper'** to  mention this paper throughout this notebook. To completely understand the Encoder Transformer, it is imperative to understand Self Attention which is used inside Multi-head Attention. The data after passing the TextVectorization layer and Embedding layer will pass the Self Attention layer.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Transformer.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121454Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=09cbffe1127e548be59acd38e8417c53fcd4aee6c145ba1e4e8bf425829c05c3\" width=750px/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "xEAlgHlEi360"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention\n",
        "The attention mechanism being depicted in the picture below can be understood as the attention scores highlighting the most important features of the cat so that it can be identified."
      ],
      "metadata": {
        "id": "OoxkpGwjLs8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Attention%20scores%20pic.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121521Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=c12f214510738487035fb8576877592d09eb5c3168109406bcd56a747f47ae6a\" width=700px/>\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "hXf1hMTsaybL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the popular language models does not have just the term 'BERT' in their name but an important techique called 'self-attention'. Transformer-based architectures, which are primarily used in modelling language understanding tasks, eschew recurrence in neural networks and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs."
      ],
      "metadata": {
        "id": "9WM_eM6KLGSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Self%20Attention%20Scores.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121558Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=58cb1ff4e1ae5aa2d72a4689060f036f05542a03071f257adefd669783a9bc67\" width=900px/>\n",
        "</center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M_nEW1yj8Gzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputs = sum(inputs * pairwise_scores(inputs, inputs))\n"
      ],
      "metadata": {
        "id": "3wTkPQOSJ7VJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the self attention scores which are depicted in the picture, the word 'train pays' more attention to station rather than other words in consideration such as 'on' or the."
      ],
      "metadata": {
        "id": "tC2wZV8FNpuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input)."
      ],
      "metadata": {
        "id": "qOp6pGTsNnP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Inside each attention head is a **Scaled Dot Product Self-Attention** operation, the operation returns a Attention vector as given by equation below:\n",
        "\n",
        "$$ Self Attention = softmax(\\frac{x^{T}_i x_j}{\\sqrt{d_k}})x_j $$\n",
        "\n",
        "The term  **$x^{T}_i x_j$** is dot product of input vector with itself. The  'pivot_vector' and the 'vector' forms the 'xi' and 'xj' of the above Self Attention function."
      ],
      "metadata": {
        "id": "gFRVERe-bRFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstrating self_attention with dummy data"
      ],
      "metadata": {
        "id": "y5OQMPqTOGRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a custom self attention function"
      ],
      "metadata": {
        "id": "weO9uy0NYcGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom self attention function\n",
        "def self_attention(input_sequence):\n",
        "  output = np.zeros(shape=input_sequence.shape)\n",
        "  for i, pivot_vector in enumerate(input_sequence): # iterate over each token in ip seq\n",
        "    scores = np.zeros(shape=(len(input_sequence), ))\n",
        "\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      scores[j] = np.dot(pivot_vector, vector.T)    # Pairwise scores\n",
        "\n",
        "    scores /= np.sqrt(input_sequence.shape[1]) # scale #[1] is the embedding dim\n",
        "    scores = tf.nn.softmax(scores)              # softmax\n",
        "    new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      new_pivot_representation += vector*scores[j] # weigthed sum\n",
        "    output[i] = new_pivot_representation\n",
        "  return output\n",
        "\n",
        "# Optional HW: Add to the code to print the attention_score matrix"
      ],
      "metadata": {
        "id": "tN4mtFwH7bWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use a dummy data and find its vectors"
      ],
      "metadata": {
        "id": "qf8cvT-qYoDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data first passes through the TextVectorization layer then through the Embedding layer and later the Self Attention scores are calculated"
      ],
      "metadata": {
        "id": "__9n0HmEZBoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, vectorize raw text using _________\n",
        "dummy_vocab = [\"movie was very nice\", \"film was good\"]\n",
        "text_vec = layers.TextVectorization(max_tokens=5, output_sequence_length=3)\n",
        "text_vec.adapt(dummy_vocab)\n",
        "print(text_vec(\"movie\"))\n",
        "#Q: why the zeros ? why shape 3?\n",
        "#Q: output type?"
      ],
      "metadata": {
        "id": "KjBz5gZ_qxrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f643d3-c697-417f-ba1c-3f111f470327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 0 0], shape=(3,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then obtain embeddings from text indices using the Embedding layer\n",
        "int_text = text_vec([\"movie was good\"])\n",
        "print(int_text)\n",
        "embedding = layers.Embedding(input_dim=5, output_dim=4)(int_text)  # Why 5 ?\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "FBVu6T8KSFVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89500c34-6b1f-4481-d77f-bd1d932da31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[1 2 1]], shape=(1, 3), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[[-0.04491154  0.00991388 -0.01688591 -0.00087849]\n",
            "  [-0.00269227  0.03428583  0.03806682 -0.03065678]\n",
            "  [-0.04491154  0.00991388 -0.01688591 -0.00087849]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output from the attention function"
      ],
      "metadata": {
        "id": "h73JajZDY0xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute output of attention module\n",
        "attention_outputs = self_attention(embedding[0].numpy())\n",
        "print(attention_outputs.shape)\n",
        "print(attention_outputs)\n",
        "# These are z1, z2, z3 in the picture below"
      ],
      "metadata": {
        "id": "9Orj9lkLqxuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7b4b53-c957-4839-838a-6ddd2d156ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "[[-0.03085044  0.01803095  0.00141606 -0.01079613]\n",
            " [-0.03082096  0.01804796  0.00145442 -0.01081692]\n",
            " [-0.03085044  0.01803095  0.00141606 -0.01079613]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Eqn. with Queries, Keys and Values\n",
        "\n",
        "We computed the Self Attention based on the inputs of vectors themselves. This means that for fixed inputs, these attention weights would always be fixed. In other words, there are no learnable parameters. Need to introduce some learnable parmeters which will make the self attention mechanism more flexible and tunable for various tasks. To fullfil this purpose, three weight matices are introduced and multiplied with input $x_i$ seperately and three new terms **Queries(Q), Keys(K) and Values(V)** comes into picture as given by equations below. Vectorized implemenation  & Shape tracking are also shown along with equations."
      ],
      "metadata": {
        "id": "xEFymWCLeAQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorized implemenation  & Shape tracking**\n",
        "\n",
        "$ d_{model} $ = Embedding vector for each word ( 512 as per the paper).\n",
        "\n",
        "$ X   \\Rightarrow (T \\times d_{model}) $\n",
        "\n",
        "\n",
        "$ Q = X W^{Q}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_k  )  \\Rightarrow   (T \\times d_{k}) $\n",
        "\n",
        "\n",
        "$ K = X W^{K}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_k  )  \\Rightarrow   (T \\times d_{k}) $\n",
        "\n",
        "\n",
        "$ V = X W^{V}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_v  )  \\Rightarrow   (T \\times d_{v}) $\n",
        "\n",
        "Dot product of Queries and Keys:\n",
        "\n",
        "$ Q K^{T}   \\Rightarrow (T \\times d_{k}) \\times (d_{k} \\times T  )  \\Rightarrow   (T \\times T) $\n",
        "\n",
        "T query vectors and T key vectors (Input Sequence), so need TxT attention weights. Make Sense! Taking SoftMax doesn't change the shape.\n",
        "\n",
        " **Shapes as per the paper**\n",
        "\n",
        "$\n",
        "\\begin{array}{|c|c|} \\hline\n",
        "Object   &  Shape & values  \\\\ \\hline\n",
        "q_i, k_i  &  d_k  &  (64,) \\\\\n",
        "v_i   &   d_v   &   (64,)  \\\\\n",
        "x_i   &   d_{model}   & (512,)  \\\\\n",
        "W^{Q}, W^{K}  &   d_{model} \\times d_k   &   (512, 64)  \\\\\n",
        "W^{V}   &   d_{model} \\times d_v   &  (512,64)  \\\\ \\hline\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**Batch consideration**\n",
        "\n",
        "In code, a batch of N samples are processed at a time. Everyting would be  **N times**, like: $ N \\times T \\times d_k $ instead of just $ T \\times d_k$."
      ],
      "metadata": {
        "id": "nEqmUxkDf-fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Fianl Scaled Dot Product Attention** equation inside each attention head with **Queries(Q)**, **Keys(Q)**, and **Values(V)**, which returns a Attention vector.\n",
        "\n",
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Scaled_dot_product_Attention.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121701Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=71d63efaa3df74c8e0da39e65f8ffb379c7e44bfdefe2b675cf999c89013afe1\" width=250px/>\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T)}{\\sqrt{d_k}})V$$"
      ],
      "metadata": {
        "id": "OHr_TAfbgdig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multihead Attention"
      ],
      "metadata": {
        "id": "6PVKxy5PcCYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the output of the final Encoder in the stack is passed to the Value and Key parameters in the Encoder-Decoder Attention.\n",
        "\n",
        "The Encoder-Decoder Attention is therefore getting a representation of both the target sequence (from the Decoder Self-Attention) and a representation of the input sequence (from the Encoder stack). It, therefore, produces a representation with the attention scores for each target sequence word that captures the influence of the attention scores from the input sequence as well.\n",
        "\n",
        "As this passes through all the Decoders in the stack, each Self-Attention and each Encoder-Decoder Attention also add their own attention scores into each words representation."
      ],
      "metadata": {
        "id": "yLiXoAWEaQS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Transformer, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined together to produce a final Attention score. This is called Multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word."
      ],
      "metadata": {
        "id": "xzQy3FMLaj8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, the diagram for a scaled dot product attention does not use any weights at all. Instead, the weights are included only in the multi head attention block, shown in figure below :\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Multi_head_attention_with_weights.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121757Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=4b7c3816ff2448c13ba5d6c4b14b560980c8415abff5470d1bc6de9709c51ef9\" width=900px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "xgX4dIqzalYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the shape\n",
        "<br>\n",
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Multi_head_attention_shape_tracking.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121829Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=ea2f94a2fa7ea1766ffa69ac4411b8b175264e693af6b93b1aa1fac4795b324c\" width=750px>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "**Final Projection :** $ Output = concat(A_1, A_2, ..., A_h)  W^{o} $\n",
        "\n",
        "**Shape of :**  $ concat (A_1, A_2, ..., A_h) \\Rightarrow  (T \\times hd_v) $\n",
        "\n",
        "**Shape of:**  $  W^{o} \\Rightarrow (hd_v \\times d_{model}) $\n",
        "\n",
        "**Shape of final:**  $ Ouput = concat (A_1, A_2, ..., A_h) W^{o} \\Rightarrow  (T \\times hd_v) \\times (hd_v \\times d_{model})  \\Rightarrow  (T \\times d_{model}) \\Leftarrow $ **Back to the initial input shape.**\n",
        "\n",
        "Batch size is not displayed here."
      ],
      "metadata": {
        "id": "QydYglmilY9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Block\n",
        "\n",
        "The Transformer Encoder consists of a stack of\n",
        " identical layers (Encoder Block) as shown in figure below, where each layer further consists of two main sub-layers:\n",
        "\n",
        "* The first sub-layer comprises a multi-head attention mechanism that receives the queries, keys, and values as inputs.\n",
        "* A second sub-layer comprises a fully-connected feed-forward network.\n",
        "\n",
        "Following each of these two sub-layers is layer normalization, into which the sub-layer input (through a residual/skip connection) and output are fed.\n",
        "\n",
        "Regularization is also intrpduced into the model by applying a dropout to the output of each sub-layer (before the layer normalization step), as well as to the positional encodings before these are fed into the encoder.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Encoder_tfr_block_unfolded.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T121853Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=8dffbe0098fe5ba23a986a7e4b96c7cd17358c780d5d3f8d5756b7e617020f20\"  width=600 px />$$\n",
        "<img src=\"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Encoder_tfr_block2.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T122026Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=21fd1c5bdd771fd7bcc8f7e051aa3f39f48ac51289724dba5207dc072d508204\" width=180 px/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "9ei6127oCBQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer encoder architecture typically consists of multiple layers, each of which includes a self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to weigh the importance of different input sequence parts by calculating the embeddings' dot product. This mechanism is also known as multi-head attention.\n",
        "\n",
        "The feed-forward network allows the model to extract higher-level features from the input. This network usually comprises two linear layers with a ReLU activation function in between. The feed-forward network allows the model to extract deeper meaning from the input data and more compactly and usefully represent the input.In the paper, an ANN with one hidden layer and a ReLu activation in the middle  with no activation function at output layer has been implemented.\n",
        "\n",
        "The transformer encoder is a crucial part of the transformer encoder-decoder architecture, which is widely used for natural language processing tasks."
      ],
      "metadata": {
        "id": "UU0hqgB0cz17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Transformer\n",
        "Stacking transormer blocks gives a Transfomer! Shown in figure below:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src= \"https://datascienceimages.s3.eu-north-1.amazonaws.com/Text_Classification_using_Transformer_Encoder_Model/Encoder_transfomer.png?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHQaCmV1LW5vcnRoLTEiRzBFAiEAmOZQhT%2BYM40tIrgSCC0juQ3jYb2AAM%2F1Fbz6dD8McLICIHg%2Fvrzjk8lvADAhmWpZrYL9TF3PwvkzKpdWA8hZeRQoKu0CCM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQABoMOTc1MDUwMDY4NjU4IgyeGfj2LlL0RNsVYPMqwQJM4d9h4CmJwEyTBC7qZEzc29kl0a%2FrmErIOHAztB%2FrvmsLwfZSviafkN0S0Rgnb9ZUlQLlOLgmJAh%2Boo5wqCAD6Vph5HKDP0UmiithH%2BlXOUIiDE51ykTUEA3w1iSSHMyOs7vDBsLp%2FMFpvpCG3V2XnlQ5Iqgo422ee763aDs%2FzuanCRZpfGDaLTo%2BgTWY0vQntjWJILYrZeOw0tpLpzZArag7KROagc0J8JjBvK0M%2Fz02ApRdrd4sspTDVFraDlGzFJekxjfCDEkf0gSQy7gutoZ2hujhd%2BonPc2Sob988wEY1BUTJWathwBoXphrbMZIS6LhUp1rTnDl8w7hv54XZ4ORo9w4oXVsZdG%2FDbp2dUX6yVHFY3mjMGdU6fEQ1j2AlzySCTm4T%2FzJpXst4CisnbG6opNd6Os2b78VkYfQnKAwtdPmsQY6swKV5d%2BjoEU9poWR0HUudhbWdbIOfv1xKG64htIRW3NpNgi0OonJi5zOPTw5aWoAvi5hbnA7akONMJtwKRTgl9t30qtpFA8DwvmNL%2BUt%2FNsayqUy%2BrKPtD%2FtfVYyu82iXyxz4EyCSXPTgcCREW61a0z%2F8HtRH76JayEUAc7PAypjS861xn7SFWDzldlYWE8H2bDvjOQdf82VBKm67cP5CPWaxKpPkpTJFAR82hOQeFordG1wctY1YO5AOZnqv8Wd%2B7Tz9rQYNYk9xUihZZHgrvYzMuQWKcj%2By2eMBR7uWxP5LR48KBnL6edR4ZCv5H5LYCGjIREzJAImRO9qbDptQLHW%2B943UkCxOSqNdCIQjgRoWe84M93lB6rTQjLowcqWYU9YJDY9V73Jfw8Qz3mx6HxtnhdV&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240507T122049Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIA6GBMDKKZK2QSB44A%2F20240507%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=245e381bff57c2e4df21da042b17752d3ecc62827c71136057aa80954b8b061a\" width=1000px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "RWF4OyFonkHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define TransformerEncoder class"
      ],
      "metadata": {
        "id": "MxZdesd1dgAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim    # Dimension of embedding. 4 in the dummy example\n",
        "        self.dense_dim = dense_dim    # No. of neurons in dense layer\n",
        "        self.num_heads = num_heads    # No. of heads for MultiHead Attention layer\n",
        "        self.attention = layers.MultiHeadAttention(# MultiHead Attention layer -\n",
        "            num_heads=num_heads, key_dim=embed_dim)   # see coloured pic above\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]    # encoders are stacked on top of the other.\n",
        "        )                                 # So output dimension is also embed_dim\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # Call function based on figure above\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]   # Will discuss in next tutorial\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)  # Query: inputs, Value: inputs, Keys: Same as Values by default\n",
        "                                                  # Q: Can you see how this is self attention?\n",
        "        proj_input = self.layernorm_1(inputs + attention_output) # LayerNormalization; + Recall cat picture\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)  # LayerNormalization + Residual connection\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "loHOqli9qxza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model definition"
      ],
      "metadata": {
        "id": "QX4opYwUdrtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Transformer encoder\n",
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "61zmVZv8q4AO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307dd4ec-d6ef-40eb-facc-be33cf6619c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5664033 (21.61 MB)\n",
            "Trainable params: 5664033 (21.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate the performance of the model"
      ],
      "metadata": {
        "id": "RwJwGlMPdypN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "GEs7SvpGq6VU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db34a3a-6073-4242-dfb3-6c3adbf5a226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "246/625 [==========>...................] - ETA: 32s - loss: 0.6247 - accuracy: 0.6951"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embedding\n",
        "\n",
        "**Positional Embedding = Word Embedding + Positional Encoding**\n",
        "\n",
        "**Positional Encoding**\n",
        "\n",
        "Passing embeddings directly into the transformer block results in missing of information about the order of tokens. As attention is permutation invariant i.e. order of token does not matter to attention.\n",
        "Although transformers are a sequence model, it appears that this important detail has somehow been lost. Positional encoding is for rescue.\n",
        "\n",
        "Positional encoding add positional information to the existing embeddings.\n",
        "\n",
        "**A unique set of numbers added at each position of the existing embeddings**, such that this new set of numbers can uniquely identify which postion they are located at.\n",
        "\n",
        "\n",
        "1. Positional Encoding by SubClassing the Keras Embedding Layer (Trainable)\n",
        "2. Positional Encoding scheme as per the paper (Non-Trainable)\n",
        "\n",
        " In this scheme the encoding is created by using a set of sins and cosines at different frequencies. The  paper uses the following formula for calculating the positional encoding. [Positional Encoding Vizualization.](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "\n",
        "We are going to implement Positional Encoding by SubClassing the Keras Embedding Layer (Trainable). Thus Instead of using the Embedding layer from keras define a PositionalEmbedding class and create a new model using it as the embedding layer."
      ],
      "metadata": {
        "id": "jFZ23Yzywtqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using positional encoding to re-inject order information\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "B1MVsKFZsVPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Definition"
      ],
      "metadata": {
        "id": "ym7HzyaLeSrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Combining the Transformer encoder with positional embedding\n",
        "\n",
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BOFIwLWGsk_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate the model"
      ],
      "metadata": {
        "id": "1_Ua8m85eWw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dro7yrtYtP5b"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "If you are very interested or plan to work closely on Transformers, then following are good resource that explains in simplified manner.\n",
        "\n",
        "1. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v6.pdf)\n",
        "\n",
        "2. [Understanding Positional  Encoding](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n",
        "\n",
        "2. [Implement Multi-Head Attention](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)\n",
        "\n",
        "3. [Implementing the Transformer Encoder](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/)\n",
        "\n",
        "4. [Illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "beX9CfsVI3on"
      }
    }
  ]
}