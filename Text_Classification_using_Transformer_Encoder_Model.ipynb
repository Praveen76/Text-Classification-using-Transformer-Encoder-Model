{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveen76/Text-Classification-using-Transformer-Encoder-Model/blob/main/Text_Classification_using_Transformer_Encoder_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the big picture of transformers\n",
        "* understand and work with the Textvectorization layer\n",
        "* understand and work with the embedding layer\n",
        "* understand the consept of self attention\n",
        "* explore transformer encoder and positional embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Big Picture"
      ],
      "metadata": {
        "id": "Tv8TQrtfwaNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST%205%20Big%20Picture.png\" width=800px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "ni6YCBwE7d_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the entire architecture of transformer. A TextVectorization layer, Embedding layer, an Encoder and a Decoder.\n",
        "\n",
        "Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence."
      ],
      "metadata": {
        "id": "AxzUg64Jdv-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer architecture was originally designed for translation. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
        "\n",
        "To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3."
      ],
      "metadata": {
        "id": "rgVPmcTjnORs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Image1_Transformer.png\" width=900px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "VtUillZR7eEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment decoder will not form the topic of discussion, the main focus will be on the Transformer Encoder.\n",
        "This has been discussed in detail in the later sections of this notebook."
      ],
      "metadata": {
        "id": "geRywC9LeHjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description"
      ],
      "metadata": {
        "id": "ECysu18IoMaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews."
      ],
      "metadata": {
        "id": "XLrl_aUaoPam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is processed and used in the later sections of this notebook."
      ],
      "metadata": {
        "id": "KzZ2UaHyoS6z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bf0671ae-abc8-4930-b33b-6bb4ad2e3b1a"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_03_Transformer_Encoder_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "    ipython.magic(\"sx tar -xvzf aclImdb_v1.tar.gz\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2302112&recordId=2648\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os, pathlib, shutil, random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import text_dataset_from_directory"
      ],
      "metadata": {
        "id": "qnUhfGmx9cup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Part A** : Text Pre-processing and Embedding Before Transformer Block"
      ],
      "metadata": {
        "id": "4F75jcgTzupQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextVectorization"
      ],
      "metadata": {
        "id": "KtDuXYqa7cbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the text data:\n",
        "  * Text standardization\n",
        "  * Text splitting\n",
        "  * Vocabulary indexing\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "TIv-Yv3EnQ4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A flowchart depicting the procedure or sequence of steps followed by a TextVectorization layer. 'Standardization' is taking care of basic preprocessing of text data such as removing the punctuation and converting the text to lower case. 'Tokenization' is giving the list of words from the sentence. Later, these words are represented with indices and with the help of embedding to get the vector encoding of indices."
      ],
      "metadata": {
        "id": "o590tI2BldJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Transformer_Encoder_Text_data_prep.png\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "y-m7SHDzoHHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All these steps are performed in a TextVectorization Layer.\n",
        "\n",
        "\n",
        "*   Keras provides a TextVectorization layer which can be dropped directly into\n",
        "      - a tf.data pipeline **or**\n",
        "      - a Keras model\n",
        "\n",
        "*  MOREOVER, TextVectorization also handles both approaches of representing groups of words:\n",
        "      - Words as a set or Bag-of-words\n",
        "      - Words as a sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_RGm-7ySooG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a dummy dataset and a test sentence\n"
      ],
      "metadata": {
        "id": "8QdHTVk5K_wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "# dataset_t = [\"I write, rewrite, and still rewrite again\"]\n",
        "#Q: Is the word 'still' in the dataset (vocabulary)? Is it there in the test_sentence?\n",
        "#Q: How many words in test_sentence?"
      ],
      "metadata": {
        "id": "njuS3JqcS-RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function depicting TextVectorization layer"
      ],
      "metadata": {
        "id": "M9sp6faVpYYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function is defined here to demonstrate the working of a TextVectorization layer. This function also compares two text data by making use of encodings and decodings."
      ],
      "metadata": {
        "id": "pB0aHkyhr3D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will be called multiple times in the sub-sequent code cells to demonstrate TextVectorization with different parameters such as a combinations of monograms, bigrams and different modes of output."
      ],
      "metadata": {
        "id": "mwf1NCofvA_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the workings of TextVectorization\n",
        "def demonstrate_TxVec(text_vectorization, dataset, test_sen, mode=None):\n",
        "  # arguments:\n",
        "  text_vectorization.adapt(dataset) # Computes a vocabulary of string terms from tokens in a dataset\n",
        "  vocabulary = text_vectorization.get_vocabulary()\n",
        "  print(f\"vocabulary = {vocabulary}\")\n",
        "  print(f\"len(vocabulary) = {len(vocabulary)}\")\n",
        "\n",
        "  # To see how the the text_vec layer transforms/vectorizes the raw text\n",
        "  encoded_sentence = text_vectorization(test_sen)\n",
        "  print(f\"encoded sentence = {encoded_sentence}\")\n",
        "  print(f\"len(encoded sentence) = {len(encoded_sentence)}\")\n",
        "  # print(f\"encoded dataset_t = {text_vectorization(dataset_t)}\")\n",
        "\n",
        "  # decode back for comparison with test_sentence\n",
        "  if mode==\"int\":\n",
        "    inverse_vocab = dict(enumerate(vocabulary)) # making a dictionary to decode embeddings\n",
        "    print(f\"inverse_vocab = {inverse_vocab}\")\n",
        "    decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "    print(f\"decoded sentence = {decoded_sentence}\")\n",
        "\n",
        "  if mode==\"multi_hot\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  if mode==\"count\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  if mode==\"tf\":\n",
        "    for token in vocabulary:\n",
        "      print(f\"{token}:\\t {text_vectorization(token)}\")\n",
        "\n",
        "  print(f\"test_sentence = {test_sen}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eqL6sCaWTnTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer"
      ],
      "metadata": {
        "id": "IMKHL4rarg_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q: What 3 things does a TV layer do?\n",
        "# instantiating a TextVectorization layer/object\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",  # int is defualt. We will see different kinds of modes\n",
        "    # we can use custom fucntions for standardizing and splitting the text - see Chollet\n",
        "    # standardize=custom_standardization_fn,\n",
        "    # split=custom_split_fn,\n",
        ")\n",
        "\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"int\")\n",
        "\n",
        "# Q: What is a vocabulary?\n",
        "# Q: No. of tokens in vocabulary?\n",
        "# Q: Length of encoded_sentence (output of TV layer)?\n",
        "# Q: Type of elements in encoded_sentence (embedding)?\n",
        "# Q: Is decoded sentence the same as the test_sentence? Why?\n",
        "\n",
        "# dataset = [\n",
        "#     \"I write, erase, rewrite\",\n",
        "#     \"Erase again, and then\",\n",
        "#     \"A poppy blooms.\",\n",
        "# ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0AS4G6RqMEU",
        "outputId": "3a486d2d-bb7e-4e6e-b3ef-2843019e273c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 12\n",
            "encoded sentence = [ 7  3  5  9  1  5 10]\n",
            "len(encoded sentence) = 7\n",
            "inverse_vocab = {0: '', 1: '[UNK]', 2: 'erase', 3: 'write', 4: 'then', 5: 'rewrite', 6: 'poppy', 7: 'i', 8: 'blooms', 9: 'and', 10: 'again', 11: 'a'}\n",
            "decoded sentence = i write rewrite and [UNK] rewrite again\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer and considering bigrams"
      ],
      "metadata": {
        "id": "yd_qVVlIvr5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with integer encoding\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    output_mode=\"int\",\n",
        "    # output_sequence_length=20\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"int\")\n",
        "# Q: Can we have 'rewrite erase' in vocab?\n",
        "# Q: Why the extra 'i write'?\n",
        "# Q: Why the extra 'UNK' ?\n",
        "\n",
        "# dataset = [\n",
        "#     \"I write, erase, rewrite\",\n",
        "#     \"Erase again, and then\",\n",
        "#     \"A poppy blooms.\",\n",
        "# ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jepK4bjqWq1D",
        "outputId": "732644b1-9704-492b-e1b1-981d167a8fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['', '[UNK]', 'erase', 'write erase', 'write', 'then', 'rewrite', 'poppy blooms', 'poppy', 'i write', 'i', 'erase rewrite', 'erase again', 'blooms', 'and then', 'and', 'again and', 'again', 'a poppy', 'a']\n",
            "len(vocabulary) = 20\n",
            "encoded sentence = [10  4  6 15  1  6 17  9  1  1  1  1  1]\n",
            "len(encoded sentence) = 13\n",
            "inverse_vocab = {0: '', 1: '[UNK]', 2: 'erase', 3: 'write erase', 4: 'write', 5: 'then', 6: 'rewrite', 7: 'poppy blooms', 8: 'poppy', 9: 'i write', 10: 'i', 11: 'erase rewrite', 12: 'erase again', 13: 'blooms', 14: 'and then', 15: 'and', 16: 'again and', 17: 'again', 18: 'a poppy', 19: 'a'}\n",
            "decoded sentence = i write rewrite and [UNK] rewrite again i write [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as integer, maximum tokens as 20 and output mode as `'multi_hot'` encodings"
      ],
      "metadata": {
        "id": "dEz-sSvJvyoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Unigrams with binary encoding\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1, # Default value\n",
        "    max_tokens = 20, # let's change this value to 8 and see\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"multi_hot\")\n",
        "\n",
        "# Recall we saw this in tutorial 2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UXYhaDYF9xg",
        "outputId": "0610ce4e-6fd5-4a15-a1d4-d0e70fbf9a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'multi_hot'` and considering bigrams"
      ],
      "metadata": {
        "id": "-fR0OClawQIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2, # bag of 2 or less words. See cell o/p\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "# Bi-grams typically perform better than unigrams- Word order matters!\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"multi_hot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJnKVKRhGk-E",
        "outputId": "ae6eca59-7e2d-4346-f23b-536de54bbf93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write erase', 'write', 'then', 'rewrite', 'poppy blooms', 'poppy', 'i write', 'i', 'erase rewrite', 'erase again', 'blooms', 'and then', 'and', 'again and', 'again', 'a poppy', 'a']\n",
            "len(vocabulary) = 19\n",
            "encoded sentence = [1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            "len(encoded sentence) = 19\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write erase:\t [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "poppy blooms:\t [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "i write:\t [0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase rewrite:\t [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase again:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "and then:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "again and:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "a poppy:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'count'` and considering monograms"
      ],
      "metadata": {
        "id": "_X9DrVGTweV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with token counts\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1,\n",
        "    # max_tokens = 8,\n",
        "    output_mode=\"count\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"count\")\n",
        "#Q: which token comes twice in the test_sentence?\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_itfCn0aGreY",
        "outputId": "13d00e7a-1455-46eb-d3d6-0446b08f8dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7ac55b75a7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [1. 0. 1. 0. 2. 0. 1. 0. 1. 1. 0.]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "erase:\t [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "write:\t [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "then:\t [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "rewrite:\t [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "poppy:\t [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "i:\t [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "blooms:\t [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "and:\t [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "again:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "a:\t [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating a TextVectorization object with output mode as `'tf_idf'` and considering monograms"
      ],
      "metadata": {
        "id": "ywFrwXxLwlD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigrams with TF-IDF weighted outputs\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 1,\n",
        "    output_mode=\"tf_idf\",\n",
        ")\n",
        "demonstrate_TxVec(text_vectorization, dataset, test_sentence, mode=\"tf\")\n",
        "\n",
        "# Often leads to 1% increase in perfermonance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3rAdK92Gyfq",
        "outputId": "974b24c3-1cc9-40ac-9f63-239921f701f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7ac55b75b9a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary = ['[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
            "len(vocabulary) = 11\n",
            "encoded sentence = [0.8939764  0.         0.91629076 0.         1.8325815  0.\n",
            " 0.91629076 0.         0.91629076 0.91629076 0.        ]\n",
            "len(encoded sentence) = 11\n",
            "[UNK]:\t [0.8939764 0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.       ]\n",
            "erase:\t [0.        0.6931472 0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.       ]\n",
            "write:\t [0.         0.         0.91629076 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "then:\t [0.         0.         0.         0.91629076 0.         0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "rewrite:\t [0.         0.         0.         0.         0.91629076 0.\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "poppy:\t [0.         0.         0.         0.         0.         0.91629076\n",
            " 0.         0.         0.         0.         0.        ]\n",
            "i:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.91629076 0.         0.         0.         0.        ]\n",
            "blooms:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.91629076 0.         0.         0.        ]\n",
            "and:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.91629076 0.         0.        ]\n",
            "again:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.91629076 0.        ]\n",
            "a:\t [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.91629076]\n",
            "test_sentence = I write, rewrite, and still rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextVectorization in Keras Model\n",
        "\n",
        "This technique is useful for production: For a stand-alone model.\n",
        "\n",
        "Load a saved model and add a 'text_vectorization' layer to it"
      ],
      "metadata": {
        "id": "yCQHEwZkFXmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a model architecture in a function"
      ],
      "metadata": {
        "id": "6QOfDPiNjTfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The processed inputs from will be later fed to this model."
      ],
      "metadata": {
        "id": "lyQ2i5GIWxDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense network which may be used repetitively\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                    loss=\"binary_crossentropy\",\n",
        "                    metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "TdyrQSXQjZEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  model=get_model()\n",
        "  inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "  processed_inputs = text_vectorization(inputs)\n",
        "  outputs = model(processed_inputs) #some trained model\n",
        "  inference_model = keras.Model(inputs, outputs)\n",
        "  inference_model.summary()\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "ImGc3tyXLgfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A discussion related to the method depicted above will be demonstrated in the next assignment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ILbGkb8LFYCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation Example"
      ],
      "metadata": {
        "id": "UkoDTKmd900K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pre-processed version of the IMDB dataset provided by Keras was used in the previous assignments."
      ],
      "metadata": {
        "id": "3XIAoz7enlj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally IMDB dataset contains the *train* and the *test* folders.\n",
        "Here, the original dataset will be used and pre-processing related to it will be explored."
      ],
      "metadata": {
        "id": "xcKJhCGE1yTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List subdirectories\n",
        "!cd aclImdb && ls -d */"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46rYfHyZrqbs",
        "outputId": "d0c1e70b-43ca-4264-f241-113a30d55779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test/  train/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary folder\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "mFgcefQNLh-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise a sample\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DWQXmaYLuha",
        "outputId": "be432e6d-6b5b-4a2a-d601-63ed42f3dd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a validation directory and move 20% of the train data to it"
      ],
      "metadata": {
        "id": "Za7BufC_2q_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# move 20% of the training data to the validation folder\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    # random.Random(1337).shuffle(files) # We should shuffle. Only commenting for demonstration\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "metadata": {
        "id": "9BseO3_sLukB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create batches of data using `text_dataset_from_directory`"
      ],
      "metadata": {
        "id": "YBeeGi5S8pbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset using utility\n",
        "batch_size = 32\n",
        "\n",
        "# Q: Name other such utilities seen earlier ?\n",
        "train_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size)\n",
        "\n",
        "val_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size)\n",
        "\n",
        "test_ds = text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size)\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) #replace x,y with x. That is remove labels, just keep text data.\n"
      ],
      "metadata": {
        "id": "cDLoEh2CLwB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618cfed3-bd32-4fd6-d190-e1fc50e7a250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 20000, 5000, and 25000 records in train, validation, and test directories with two class as positive and negative."
      ],
      "metadata": {
        "id": "htOQeKY98_3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shapes\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "\n",
        "    print(\"inputs[2]:\", inputs[2])\n",
        "    print(\"targets[2]:\", targets[2])\n",
        "    break"
      ],
      "metadata": {
        "id": "u_03-Oj9LwD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a84bfd-236e-47ed-d29b-577526516704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[2]: tf.Tensor(b\"Pretty good film from Preminger; labyrinthine at times, as it explores sets and locales from various angles and perspectives as if it were a nature film on the denizens of the modern city and how they live. In this sense it is visually and spatially satisfying, as its hero, a good cop with a bad temper, gets into very hot water when he accidentally kills a guy with a plate in his head.<br /><br />Dana Andrews plays the lead as if it were Hamlet, and has never been better. The story may be pure melodrama but Andrews gives it weight, and almost raises it to the level of tragedy. As his girl, Gene Tierney is attractive but unremarkable. Gary Merrill makes for a very interesting villain, with his natural warmth providing a nice contrast to Andrews' coolness; his smiling, amiable-seeming bad guy seems to be continually challenging his nemesis by the mere fact of his being emotionally open, as opposed to the tightly wound and moralistic cop who is pursuing him. <br /><br />There are no major surprises in this film, which seems transitional for all concerned. For director Preminger it is a reunion of sorts with his Laura stars, Andrews and Tierney, who were passing their career peaks at around the time the movie was made. The supporting cast,--Merrill, Karl Malden, Neville Brand--are, understandably, more optimistic, as they were all on their way up. Preminger, as serene an observer as ever, lets the events unfold without expressing a strong point of view, as the morally ambiguous ending is somewhat disappointing, for the cat and mouse game between the two antagonists seems larger and more archetypal than any mere movie could contain, much less resolve.<br /><br />\", shape=(), dtype=string)\n",
            "targets[2]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the data using TextVectorization layer of keras"
      ],
      "metadata": {
        "id": "sR63E5Ea9fDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the data\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,    # Q: What is the vocabular size?\n",
        "    output_mode=\"int\",        # Q: What will be the type of output for a token (say), 'amazing' ?\n",
        "    output_sequence_length=max_length,      # Q: What is the maximum length of review? Is it a fair assumption?\n",
        "    )\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bqi5Z24gMK9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5_AST_05_Transformer_Encoder_Text_data_prep.png\" width=650px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "lCFPfVAts1IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize and compare the raw and processed data"
      ],
      "metadata": {
        "id": "J7sj-tcc9v2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the raw text and the vectorized (to int) text\n",
        "for text, label in train_ds:\n",
        "  print(text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "for int_of_text, label in int_train_ds:\n",
        "  print(int_of_text[0])\n",
        "  print(label[0])\n",
        "  break\n",
        "\n",
        "# Q: How can you verify whether the index of movie is 18?\n"
      ],
      "metadata": {
        "id": "nMUYGUqBx3K1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc18b46d-0b0c-4f59-be17-2d4fbaae2060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'I basically picked up this movie because I had seen Kitano Takashi\\'s brilliant remake of Zatoichi and was in the mood for another updated samurai tale which also starred Asano Tadanobu. These two movies are worlds apart. Zatoichi added humor and depth to its characters and subverted traditional samurai movie clich\\xc3\\xa9s. Gojoe goes off the deep end in the other direction.<br /><br />First off, I hate movies that have other characters inform the audience what the main character is like instead of having the character develop over the course of the movie. \"You cannot decide whether you are a monk or a warrior\" says almost every character in Benkei\\'s presence, yet this inner turmoil is barely conveyed within the character himself. Instead of character development, we get bloated, boring, gory battle scenes. Asano\\'s character is undeveloped and even he looks like he is bored and doesn\\'t know what he is doing there. I know that he usually looks distant and cool and that is part of Asano\\'s appeal, but this movie doesn\\'t serve him.<br /><br />A lot of the camera movement is nauseating. There is a scene that goes on forever in which the camera spins around the main characters until my wife and I felt like vomiting. The ending is ridiculous and rather anti-climatic. <br /><br />Its too bad that really good samurai movies aren\\'t being made in Japan nowadays with this type of budget. The colors, scenery, and costumes were great, but the rest is just a loooong waste of time. I would rather see one of the kabuki versions of this myth.', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(\n",
            "[12855  9855  1969    34   808    16    40   237     8     2    50   716\n",
            "   458    40   213  2650    56    57   117    22   130    73     3   602\n",
            "  5690   453     2    20     1   110    14     4  4365    91   343   243\n",
            "     6  1306     6   738     1   110    28   117     4   765   285    16\n",
            "    25   237   274     8  2206  7231   193   152    68    22  2266    19\n",
            "    68    53  1084     8    65   546    10   421    12   738  8699  1969\n",
            "    34   808    16    25   237    79    12     7    49 12855   219    26\n",
            "  1789    40   808    16    57   219    26    75     4  9030   519    29\n",
            "     5     2    86     6  4061   509    11   180    39   140    12   344\n",
            "   121   381  1587  2779    44   248    34  1097     3  6719    10   399\n",
            "     1   237     6    27  5409   453     3    52    57  1125    57   117\n",
            "    22   637     8  2657    36    97   921   137    57  2495     2   165\n",
            "   453     2    20    57    61  1133    40  2657    42  1417   273  1410\n",
            "    45    40   529   114    14    38  1279    10  1358    10   117    22\n",
            "    66     9     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of the word movie"
      ],
      "metadata": {
        "id": "P01-UwZIAuzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization(\"movie\")\n",
        "# Q: What is the shape of the TV output?\n",
        "# Q: Why so many 0s?\n"
      ],
      "metadata": {
        "id": "Jl8iSY55z1A0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b63cdee-0583-4ce1-fd62-2c6450174578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(600,), dtype=int64, numpy=\n",
              "array([18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0])>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector representation of \"great movie\" and \"a fine story\""
      ],
      "metadata": {
        "id": "6oAL6gzUAzvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization([\"great movie\",\"a fine story\"])\n",
        "#Q: shape?"
      ],
      "metadata": {
        "id": "oCDXUf8bDRIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655ec3ca-7a29-4799-be9b-665ea64072e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 600), dtype=int64, numpy=\n",
              "array([[ 84,  18,   0, ...,   0,   0,   0],\n",
              "       [  4, 481,  64, ...,   0,   0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "BL3w3DRqVy5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Word embeddings are vector representations of words that achieve exactly this: they map human language into a structured geometric space.\n",
        "\n",
        "* dense (floats)\n",
        "* low-dimensional (1024 dims for large vocabs)"
      ],
      "metadata": {
        "id": "ex3lQf00YyM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "* Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors, in the same way you learn the weights of a neural network. **Move away from manual feature engineering.**\n",
        "* Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called pretrained word embeddings.\n",
        "\n",
        "**Q: Do two ways remind you of something we studied in CNNs ?**\n",
        "\n",
        "In this assignment the main agenda is to explore the Learning of word embeddings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fMKg5tOpcgXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layer\n"
      ],
      "metadata": {
        "id": "t5OVct9NckRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The procedure if as follows:\n",
        "\n",
        "*   Like a dictionary that **maps integer indices** (which stand for specific words) **to dense vectors**\n",
        "\n",
        "*   Input: a rank-2 tensor of integers, of shape (batch_size, sequence_length)\n",
        "*   Output: 3D floating-point tensor of shape (batch_size, sequence_length, embedding_dimensionality)\n",
        "*   WORD INDEX ⭢ EMBEDDING LAYER ⭢ CORRESPONDING WORD VEC\n",
        "\n",
        "*   Initial weights are random\n",
        "*   Learns specialized structure upon training\n",
        "\n"
      ],
      "metadata": {
        "id": "gdLRIpXwkQAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Embedding%20Layer.png\" width=750px/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "G91_UEMy-gpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define an LSTM architecture with an Embedding layer, and a Bidriectional layer"
      ],
      "metadata": {
        "id": "K_tTVNb4GhCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 20000\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "# The Embedding layer\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)  # the largest integer (i.e. word index) in the input\n",
        "                                                                             # should be no larger than 19999 (vocabulary size).\n",
        "# Q: What is the input to the Embedding layer?\n",
        "# Q: What is the dimension of the output embeddings\n",
        "# Q: In embedding layer shape, what are None and None ?\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "#Q: Weights in the embedding layer?\n",
        "#Hint: Dict; 1 input word => embedding of size ___ ."
      ],
      "metadata": {
        "id": "njaeQikslGr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45ad17a-3b6b-4986-cb94-112c50dc012d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                73984     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5194049 (19.81 MB)\n",
            "Trainable params: 5194049 (19.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model and make a prediction"
      ],
      "metadata": {
        "id": "0wwNyu9PG4Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model - This code cell can be commented for brevity\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "fPbyk2A483fF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405fb3e4-ce76-4871-8d45-33a73fb8e8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 64s 93ms/step - loss: 0.5259 - accuracy: 0.7401 - val_loss: 0.4061 - val_accuracy: 0.8332\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.3517 - accuracy: 0.8688 - val_loss: 0.3377 - val_accuracy: 0.8612\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.2864 - accuracy: 0.8958 - val_loss: 0.3556 - val_accuracy: 0.8462\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.2317 - accuracy: 0.9190 - val_loss: 0.3543 - val_accuracy: 0.8632\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.1993 - accuracy: 0.9321 - val_loss: 0.4052 - val_accuracy: 0.8494\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.1709 - accuracy: 0.9426 - val_loss: 0.4044 - val_accuracy: 0.8550\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.1538 - accuracy: 0.9496 - val_loss: 0.4586 - val_accuracy: 0.8504\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.1314 - accuracy: 0.9590 - val_loss: 0.4274 - val_accuracy: 0.8708\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 28s 44ms/step - loss: 0.1115 - accuracy: 0.9660 - val_loss: 0.5808 - val_accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.0948 - accuracy: 0.9735 - val_loss: 0.4769 - val_accuracy: 0.8692\n",
            "782/782 [==============================] - 16s 19ms/step - loss: 0.3478 - accuracy: 0.8567\n",
            "Test acc: 0.857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **Part B** : Building Encoder Transformer"
      ],
      "metadata": {
        "id": "L4lbyeZVxojj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure below shows the **Transformer Model Architecture** as per the paper [\"**Attention Is All You Need**\"](https://arxiv.org/pdf/1706.03762v6.pdf) .Here, we are going to implement **Encoder** and try to understand how it function. We are writing **'as per the paper'** to  mention this paper throughout this notebook. To completely understand the Encoder Transformer, it is imperative to understand Self Attention which is used inside Multi-head Attention. The data after passing the TextVectorization layer and Embedding layer will pass the Self Attention layer.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Transformer.png\" width=750px/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "xEAlgHlEi360"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention\n",
        "The attention mechanism being depicted in the picture below can be understood as the attention scores highlighting the most important features of the cat so that it can be identified."
      ],
      "metadata": {
        "id": "OoxkpGwjLs8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Attention%20scores%20pic.png\" width=700px/>\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "hXf1hMTsaybL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the popular language models does not have just the term 'BERT' in their name but an important techique called 'self-attention'. Transformer-based architectures, which are primarily used in modelling language understanding tasks, eschew recurrence in neural networks and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs."
      ],
      "metadata": {
        "id": "9WM_eM6KLGSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src= \"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/M5%20AST5%20Self%20Attention%20Scores.png\" width=900px/>\n",
        "</center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M_nEW1yj8Gzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "outputs = sum(inputs * pairwise_scores(inputs, inputs))\n"
      ],
      "metadata": {
        "id": "3wTkPQOSJ7VJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the self attention scores which are depicted in the picture, the word 'train pays' more attention to station rather than other words in consideration such as 'on' or the."
      ],
      "metadata": {
        "id": "tC2wZV8FNpuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input)."
      ],
      "metadata": {
        "id": "qOp6pGTsNnP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Inside each attention head is a **Scaled Dot Product Self-Attention** operation, the operation returns a Attention vector as given by equation below:\n",
        "\n",
        "$$ Self Attention = softmax(\\frac{x^{T}_i x_j}{\\sqrt{d_k}})x_j $$\n",
        "\n",
        "The term  **$x^{T}_i x_j$** is dot product of input vector with itself. The  'pivot_vector' and the 'vector' forms the 'xi' and 'xj' of the above Self Attention function."
      ],
      "metadata": {
        "id": "gFRVERe-bRFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstrating self_attention with dummy data"
      ],
      "metadata": {
        "id": "y5OQMPqTOGRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a custom self attention function"
      ],
      "metadata": {
        "id": "weO9uy0NYcGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom self attention function\n",
        "def self_attention(input_sequence):\n",
        "  output = np.zeros(shape=input_sequence.shape)\n",
        "  for i, pivot_vector in enumerate(input_sequence): # iterate over each token in ip seq\n",
        "    scores = np.zeros(shape=(len(input_sequence), ))\n",
        "\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      scores[j] = np.dot(pivot_vector, vector.T)    # Pairwise scores\n",
        "\n",
        "    scores /= np.sqrt(input_sequence.shape[1]) # scale #[1] is the embedding dim\n",
        "    scores = tf.nn.softmax(scores)              # softmax\n",
        "    new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      new_pivot_representation += vector*scores[j] # weigthed sum\n",
        "    output[i] = new_pivot_representation\n",
        "  return output\n",
        "\n",
        "# Optional HW: Add to the code to print the attention_score matrix"
      ],
      "metadata": {
        "id": "tN4mtFwH7bWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use a dummy data and find its vectors"
      ],
      "metadata": {
        "id": "qf8cvT-qYoDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data first passes through the TextVectorization layer then through the Embedding layer and later the Self Attention scores are calculated"
      ],
      "metadata": {
        "id": "__9n0HmEZBoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, vectorize raw text using _________\n",
        "dummy_vocab = [\"movie was very nice\", \"film was good\"]\n",
        "text_vec = layers.TextVectorization(max_tokens=5, output_sequence_length=3)\n",
        "text_vec.adapt(dummy_vocab)\n",
        "print(text_vec(\"movie\"))\n",
        "#Q: why the zeros ? why shape 3?\n",
        "#Q: output type?"
      ],
      "metadata": {
        "id": "KjBz5gZ_qxrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f643d3-c697-417f-ba1c-3f111f470327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 0 0], shape=(3,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then obtain embeddings from text indices using the Embedding layer\n",
        "int_text = text_vec([\"movie was good\"])\n",
        "print(int_text)\n",
        "embedding = layers.Embedding(input_dim=5, output_dim=4)(int_text)  # Why 5 ?\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "FBVu6T8KSFVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89500c34-6b1f-4481-d77f-bd1d932da31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[1 2 1]], shape=(1, 3), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[[-0.04491154  0.00991388 -0.01688591 -0.00087849]\n",
            "  [-0.00269227  0.03428583  0.03806682 -0.03065678]\n",
            "  [-0.04491154  0.00991388 -0.01688591 -0.00087849]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output from the attention function"
      ],
      "metadata": {
        "id": "h73JajZDY0xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute output of attention module\n",
        "attention_outputs = self_attention(embedding[0].numpy())\n",
        "print(attention_outputs.shape)\n",
        "print(attention_outputs)\n",
        "# These are z1, z2, z3 in the picture below"
      ],
      "metadata": {
        "id": "9Orj9lkLqxuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7b4b53-c957-4839-838a-6ddd2d156ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "[[-0.03085044  0.01803095  0.00141606 -0.01079613]\n",
            " [-0.03082096  0.01804796  0.00145442 -0.01081692]\n",
            " [-0.03085044  0.01803095  0.00141606 -0.01079613]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Eqn. with Queries, Keys and Values\n",
        "\n",
        "We computed the Self Attention based on the inputs of vectors themselves. This means that for fixed inputs, these attention weights would always be fixed. In other words, there are no learnable parameters. Need to introduce some learnable parmeters which will make the self attention mechanism more flexible and tunable for various tasks. To fullfil this purpose, three weight matices are introduced and multiplied with input $x_i$ seperately and three new terms **Queries(Q), Keys(K) and Values(V)** comes into picture as given by equations below. Vectorized implemenation  & Shape tracking are also shown along with equations."
      ],
      "metadata": {
        "id": "xEFymWCLeAQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorized implemenation  & Shape tracking**\n",
        "\n",
        "$ d_{model} $ = Embedding vector for each word ( 512 as per the paper).\n",
        "\n",
        "$ X   \\Rightarrow (T \\times d_{model}) $\n",
        "\n",
        "\n",
        "$ Q = X W^{Q}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_k  )  \\Rightarrow   (T \\times d_{k}) $\n",
        "\n",
        "\n",
        "$ K = X W^{K}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_k  )  \\Rightarrow   (T \\times d_{k}) $\n",
        "\n",
        "\n",
        "$ V = X W^{V}   \\Rightarrow (T \\times d_{model}) \\times (d_{model} \\times d_v  )  \\Rightarrow   (T \\times d_{v}) $\n",
        "\n",
        "Dot product of Queries and Keys:\n",
        "\n",
        "$ Q K^{T}   \\Rightarrow (T \\times d_{k}) \\times (d_{k} \\times T  )  \\Rightarrow   (T \\times T) $\n",
        "\n",
        "T query vectors and T key vectors (Input Sequence), so need TxT attention weights. Make Sense! Taking SoftMax doesn't change the shape.\n",
        "\n",
        " **Shapes as per the paper**\n",
        "\n",
        "$\n",
        "\\begin{array}{|c|c|} \\hline\n",
        "Object   &  Shape & values  \\\\ \\hline\n",
        "q_i, k_i  &  d_k  &  (64,) \\\\\n",
        "v_i   &   d_v   &   (64,)  \\\\\n",
        "x_i   &   d_{model}   & (512,)  \\\\\n",
        "W^{Q}, W^{K}  &   d_{model} \\times d_k   &   (512, 64)  \\\\\n",
        "W^{V}   &   d_{model} \\times d_v   &  (512,64)  \\\\ \\hline\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "**Batch consideration**\n",
        "\n",
        "In code, a batch of N samples are processed at a time. Everyting would be  **N times**, like: $ N \\times T \\times d_k $ instead of just $ T \\times d_k$."
      ],
      "metadata": {
        "id": "nEqmUxkDf-fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Fianl Scaled Dot Product Attention** equation inside each attention head with **Queries(Q)**, **Keys(Q)**, and **Values(V)**, which returns a Attention vector.\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Scaled_dot_product_Attention.png\" width=250px/>\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T)}{\\sqrt{d_k}})V$$"
      ],
      "metadata": {
        "id": "OHr_TAfbgdig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multihead Attention"
      ],
      "metadata": {
        "id": "6PVKxy5PcCYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the output of the final Encoder in the stack is passed to the Value and Key parameters in the Encoder-Decoder Attention.\n",
        "\n",
        "The Encoder-Decoder Attention is therefore getting a representation of both the target sequence (from the Decoder Self-Attention) and a representation of the input sequence (from the Encoder stack). It, therefore, produces a representation with the attention scores for each target sequence word that captures the influence of the attention scores from the input sequence as well.\n",
        "\n",
        "As this passes through all the Decoders in the stack, each Self-Attention and each Encoder-Decoder Attention also add their own attention scores into each word’s representation."
      ],
      "metadata": {
        "id": "yLiXoAWEaQS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Transformer, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined together to produce a final Attention score. This is called Multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word."
      ],
      "metadata": {
        "id": "xzQy3FMLaj8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, the diagram for a scaled dot product attention does not use any weights at all. Instead, the weights are included only in the multi head attention block, shown in figure below :\n",
        "<br><br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Multi_head_attention_with_weights.png\" width=900px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "xgX4dIqzalYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the shape\n",
        "<br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Multi_head_attention_shape_tracking.png\" width=750px>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "**Final Projection :** $ Output = concat(A_1, A_2, ..., A_h)  W^{o} $\n",
        "\n",
        "**Shape of :**  $ concat (A_1, A_2, ..., A_h) \\Rightarrow  (T \\times hd_v) $\n",
        "\n",
        "**Shape of:**  $  W^{o} \\Rightarrow (hd_v \\times d_{model}) $\n",
        "\n",
        "**Shape of final:**  $ Ouput = concat (A_1, A_2, ..., A_h) W^{o} \\Rightarrow  (T \\times hd_v) \\times (hd_v \\times d_{model})  \\Rightarrow  (T \\times d_{model}) \\Leftarrow $ **Back to the initial input shape.**\n",
        "\n",
        "Batch size is not displayed here."
      ],
      "metadata": {
        "id": "QydYglmilY9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Block\n",
        "\n",
        "The Transformer Encoder consists of a stack of\n",
        " identical layers (Encoder Block) as shown in figure below, where each layer further consists of two main sub-layers:\n",
        "\n",
        "* The first sub-layer comprises a multi-head attention mechanism that receives the queries, keys, and values as inputs.\n",
        "* A second sub-layer comprises a fully-connected feed-forward network.\n",
        "\n",
        "Following each of these two sub-layers is layer normalization, into which the sub-layer input (through a residual/skip connection) and output are fed.\n",
        "\n",
        "Regularization is also intrpduced into the model by applying a dropout to the output of each sub-layer (before the layer normalization step), as well as to the positional encodings before these are fed into the encoder.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data//Images/Encoder_tfr_block_unfolded.png\"  width=600 px />$⇒$\n",
        "<img src=\"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Encoder_tfr_block.png\" width=180 px/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "9ei6127oCBQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer encoder architecture typically consists of multiple layers, each of which includes a self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to weigh the importance of different input sequence parts by calculating the embeddings' dot product. This mechanism is also known as multi-head attention.\n",
        "\n",
        "The feed-forward network allows the model to extract higher-level features from the input. This network usually comprises two linear layers with a ReLU activation function in between. The feed-forward network allows the model to extract deeper meaning from the input data and more compactly and usefully represent the input.In the paper, an ANN with one hidden layer and a ReLu activation in the middle  with no activation function at output layer has been implemented.\n",
        "\n",
        "The transformer encoder is a crucial part of the transformer encoder-decoder architecture, which is widely used for natural language processing tasks."
      ],
      "metadata": {
        "id": "UU0hqgB0cz17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Transformer\n",
        "Stacking transormer blocks gives a Transfomer! Shown in figure below:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src= \"https://cdn.extras.talentsprint.com/aiml/Experiment_related_data/Images/Encoder_transfomer.png\" width=1000px/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "RWF4OyFonkHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define TransformerEncoder class"
      ],
      "metadata": {
        "id": "MxZdesd1dgAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim    # Dimension of embedding. 4 in the dummy example\n",
        "        self.dense_dim = dense_dim    # No. of neurons in dense layer\n",
        "        self.num_heads = num_heads    # No. of heads for MultiHead Attention layer\n",
        "        self.attention = layers.MultiHeadAttention(# MultiHead Attention layer -\n",
        "            num_heads=num_heads, key_dim=embed_dim)   # see coloured pic above\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]    # encoders are stacked on top of the other.\n",
        "        )                                 # So output dimension is also embed_dim\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # Call function based on figure above\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]   # Will discuss in next tutorial\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)  # Query: inputs, Value: inputs, Keys: Same as Values by default\n",
        "                                                  # Q: Can you see how this is self attention?\n",
        "        proj_input = self.layernorm_1(inputs + attention_output) # LayerNormalization; + Recall cat picture\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)  # LayerNormalization + Residual connection\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "loHOqli9qxza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model definition"
      ],
      "metadata": {
        "id": "QX4opYwUdrtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Transformer encoder\n",
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "61zmVZv8q4AO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307dd4ec-d6ef-40eb-facc-be33cf6619c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5664033 (21.61 MB)\n",
            "Trainable params: 5664033 (21.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate the performance of the model"
      ],
      "metadata": {
        "id": "RwJwGlMPdypN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "GEs7SvpGq6VU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db34a3a-6073-4242-dfb3-6c3adbf5a226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "246/625 [==========>...................] - ETA: 32s - loss: 0.6247 - accuracy: 0.6951"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embedding\n",
        "\n",
        "**Positional Embedding = Word Embedding + Positional Encoding**\n",
        "\n",
        "**Positional Encoding**\n",
        "\n",
        "Passing embeddings directly into the transformer block results in missing of information about the order of tokens. As attention is permutation invariant i.e. order of token does not matter to attention.\n",
        "Although transformers are a sequence model, it appears that this important detail has somehow been lost. Positional encoding is for rescue.\n",
        "\n",
        "Positional encoding add positional information to the existing embeddings.\n",
        "\n",
        "**A unique set of numbers added at each position of the existing embeddings**, such that this new set of numbers can uniquely identify which postion they are located at.\n",
        "\n",
        "\n",
        "1. Positional Encoding by SubClassing the Keras Embedding Layer (Trainable)\n",
        "2. Positional Encoding scheme as per the paper (Non-Trainable)\n",
        "\n",
        " In this scheme the encoding is created by using a set of sins and cosines at different frequencies. The  paper uses the following formula for calculating the positional encoding. [Positional Encoding Vizualization.](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "\n",
        "We are going to implement Positional Encoding by SubClassing the Keras Embedding Layer (Trainable). Thus Instead of using the Embedding layer from keras define a PositionalEmbedding class and create a new model using it as the embedding layer."
      ],
      "metadata": {
        "id": "jFZ23Yzywtqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using positional encoding to re-inject order information\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "B1MVsKFZsVPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Definition"
      ],
      "metadata": {
        "id": "ym7HzyaLeSrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Combining the Transformer encoder with positional embedding\n",
        "\n",
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BOFIwLWGsk_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and evaluate the model"
      ],
      "metadata": {
        "id": "1_Ua8m85eWw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dro7yrtYtP5b"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "If you are very interested or plan to work closely on Transformers, then following are good resource that explains in simplified manner.\n",
        "\n",
        "1. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762v6.pdf)\n",
        "\n",
        "2. [Understanding Positional  Encoding](https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers)\n",
        "\n",
        "2. [Implement Multi-Head Attention](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)\n",
        "\n",
        "3. [Implementing the Transformer Encoder](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/)\n",
        "\n",
        "4. [Illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "beX9CfsVI3on"
      }
    }
  ]
}